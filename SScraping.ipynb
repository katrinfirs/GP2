{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9yWTYaCpnqh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "vacancy_list = []\n",
        "ccount = 0\n",
        "per_page = 50\n",
        "max_pages = 40\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "}\n",
        "industries = [7, 41, 13, 27, 36, 19, 48, 24, 47, 39, 5, 33, 42, 9, 13, 50, 15, 44, 49]\n",
        "for industry in industries:\n",
        "    for page in range(0, max_pages):\n",
        "        url = f\"https://hh.ru/search/vacancy?industry={industry}&page={page}\"\n",
        "        resp = requests.get(url, headers=headers)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Ошибка запроса: {resp.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        vacancies = soup.find_all(\"div\", re.compile(r\"^vacancy-info--\"))\n",
        "\n",
        "        if not vacancies:\n",
        "            break\n",
        "\n",
        "        for vacancy in vacancies:\n",
        "            try:\n",
        "\n",
        "                name_tag = vacancy.find('h2', attrs={'data-qa': 'bloko-header-2'})\n",
        "                if name_tag:\n",
        "                    title = name_tag.get_text(strip=True)\n",
        "\n",
        "                title_tag = vacancy.find('a', attrs={'data-qa': 'serp-item__title'})\n",
        "                if title_tag:\n",
        "                    href = title_tag['href']\n",
        "\n",
        "                addres_tag = vacancy.find('span', attrs={'data-qa': 'vacancy-serp__vacancy-address'})\n",
        "                if addres_tag:\n",
        "                    addres = addres_tag.get_text(strip=True)\n",
        "\n",
        "                price_tag = vacancy.find('div', re.compile(r\"^compensation-labels--\"))\n",
        "                if price_tag:\n",
        "                    price = price_tag.get_text(strip=True)\n",
        "\n",
        "                vacancy_list.append({\n",
        "                    'название вакансии': title or '',\n",
        "                    'зарплата': price or '',\n",
        "                    'адрес': addres or '',\n",
        "                    'url': href or ''\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "              print(\"Ошибка парсинга вакансии:\", e)\n",
        "\n",
        "        ccount= ccount + len(vacancy_list)\n",
        "        print(f\"Страница {page+1} обработана, всего {ccount} вакансий\")\n",
        "\n",
        "        df = pd.DataFrame(vacancy_list)\n",
        "        df.to_csv(\"hh_scraped_vacancies.csv\", mode='a', header=not pd.io.common.file_exists(\"hh_scraped_vacancies.csv\"), index=False)\n",
        "        vacancy_list = []\n",
        "\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        if ccount >= 10000:\n",
        "            break\n",
        "\n",
        "print(\"завершено\")"
      ]
    }
  ]
}